{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MGM_and_GMM_code.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9rm5FdBdihF4"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_xfsWlmSvvd"
      },
      "source": [
        "# **Multivariate Gaussian models and Gaussian Mixture Models** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Sa3yfYucv1k"
      },
      "source": [
        "##  Multivariate Gaussian models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_q-oveic02S"
      },
      "source": [
        "Description: <br> \n",
        "\n",
        "A simple classifier using multivariate Gaussian models. Each class in the data is modeled by a single 3D Gaussian distribution. <br>\n",
        "\n",
        "Two different structures for the covariance matrices are considered for the model, which are:\n",
        "- Each Gaussian uses a diagonal covariance matrix.\n",
        "- Each Gaussian uses a full covariance matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e-4hf7NfCr1"
      },
      "source": [
        "Model Explanation: <br>\n",
        "\n",
        "We first start by loading and preprocessing the data. The data is loaded via the read csv\n",
        "method which render the data in a dataframe. Next, we convert the dataframe into numpy\n",
        "arrays and separate the labels from the input data. The input data is further normalized for\n",
        "faster processing and the binary labels are converted to 0, 1 labels for easier processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDxj7eDjSqah",
        "outputId": "d7db50fe-01d4-4612-acbc-885966caa2e6"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_data = pd.read_csv(\"train-gaussian.csv\")\n",
        "test_data = pd.read_csv(\"test-gaussian.csv\")\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1891, 4)\n",
            "(830, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIbXmjjqyLJW",
        "outputId": "8351ea16-0e6d-42b0-a41f-8c1e198b1a9f"
      },
      "source": [
        "print(list(train_data.columns.values))\n",
        "print(list(test_data.columns.values))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['class', ' x1', ' x2', ' x3']\n",
            "['class', 'x1', 'x2', 'x3']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHXUTdDry4hU",
        "outputId": "7733e737-0f15-46f6-9a4c-aad9d64bc96d"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "Xtrain =  train_data[[' x1', ' x2', ' x3']].to_numpy().astype(np.float16)\n",
        "Xtrain = MinMaxScaler().fit_transform(Xtrain)\n",
        "print(Xtrain.shape)\n",
        "print(type(Xtrain))\n",
        "\n",
        "ytrain = train_data[['class']].to_numpy()\n",
        "ytrain = np.where(ytrain == 'A', 0, 1)\n",
        "ytrain = ytrain.astype(np.float16)\n",
        "print(ytrain.shape)\n",
        "print(type(ytrain))\n",
        "\n",
        "Xtest =  test_data[['x1', 'x2', 'x3']].to_numpy().astype(np.float16)\n",
        "Xtest = MinMaxScaler().fit_transform(Xtest)\n",
        "print(Xtest.shape)\n",
        "print(type(Xtest))\n",
        "\n",
        "ytest = test_data[['class']].to_numpy()\n",
        "ytest = np.where(ytest == 'A', 0, 1)\n",
        "ytest = ytest.astype(np.float16)\n",
        "print(ytest.shape)\n",
        "print(type(ytest))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1891, 3)\n",
            "<class 'numpy.ndarray'>\n",
            "(1891, 1)\n",
            "<class 'numpy.ndarray'>\n",
            "(830, 3)\n",
            "<class 'numpy.ndarray'>\n",
            "(830, 1)\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjz-JhCKx4_x",
        "outputId": "966ff53d-39ac-4895-8f62-8e644d9ef051"
      },
      "source": [
        "print(Xtrain[0])\n",
        "print(ytrain[:10])\n",
        "print(Xtest[0])\n",
        "print(ytest[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.5977 0.571  0.6875]\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "[0.5625 0.6216 0.6323]\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkF8AjirfQFg"
      },
      "source": [
        "Now, to build a binary classifier using multivariate Gaussian models, we build a class called GDA that consists of all model functions and parameters. We initialize the model with the\n",
        "training data and separate the two classes to model their individual Gaussian distributions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDmUot5bfdXG"
      },
      "source": [
        "\n",
        "Next, for each class we compute the mean and covariance matrix for the Gaussian distribution and the diagonal elements of the full covariance matrix are taken to build the\n",
        "diagonal covariance matrix for each class. We thus complete the training part of the model\n",
        "and print the mean vectors, covariance vectors, and diagonal vectors for both classes.\n",
        "Next, we evaluate the model on the test data. The predict function takes in the test data and\n",
        "the co var mat parameter which takes the full covariance matrix for prediction if set to 0, else\n",
        "takes the diagonal covariance matrix. Thus, for every point in the test data, its probability is\n",
        "computed for both the classes, and the point is assigned to the class with a higher probability.\n",
        "This probability is computed using the MAP decision rule. We call the predict function for\n",
        "both the full covariance matrix and the diagonal covariance matrix, and compare the accuracy\n",
        "between the two. We thus obtain the following results for the experiment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfD-kQFxuFQm",
        "outputId": "3446950c-7823-4063-9792-c1006ea023f2"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class GDA:\n",
        "    def __init__(self,train_data,train_label):\n",
        "  \n",
        "        self.Train_Data = train_data\n",
        "        self.Train_Label = train_label\n",
        "        self.postive_num = 0                                                    \n",
        "        self.negetive_num = 0                                                   \n",
        "        postive_data = []                                                       \n",
        "        negetive_data = [] \n",
        "                                                             \n",
        "        for (data,label) in zip(self.Train_Data,self.Train_Label):\n",
        "            if label == 1:          \n",
        "                self.postive_num += 1\n",
        "                postive_data.append(list(data))\n",
        "            else:                   \n",
        "                self.negetive_num += 1\n",
        "                negetive_data.append(list(data))\n",
        "\n",
        "        row,col = np.shape(train_data)                                    \n",
        "        # Positive and negative samples calculated mean vector of the Gaussian distribution\n",
        "        postive_data = np.array(postive_data)\n",
        "        negetive_data = np.array(negetive_data)\n",
        "        postive_data_sum = np.sum(postive_data, 0)\n",
        "        negetive_data_sum = np.sum(negetive_data, 0)\n",
        "        self.mu_positive = postive_data_sum*1.0/self.postive_num                \n",
        "        self.mu_negetive = negetive_data_sum*1.0/self.negetive_num    \n",
        "        print(\"Class A mean: \",self.mu_negetive )\n",
        "        print(\"Class B mean: \",self.mu_positive )\n",
        "\n",
        "        # Negative sample mean vector of the Gaussian distribution\n",
        "        # Compute the covariance matrix of the Gaussian distribution\n",
        "        \n",
        "        positive_deta = postive_data-self.mu_positive\n",
        "        negetive_deta = negetive_data-self.mu_negetive\n",
        "        self.sigma = []\n",
        "        self.sigma_diag = []\n",
        "\n",
        "        for deta in positive_deta:\n",
        "            deta = deta.reshape(1,col)\n",
        "            ans = deta.T @ deta\n",
        "            self.sigma.append(ans)\n",
        "\n",
        "        for deta in negetive_deta:\n",
        "            deta = deta.reshape(1,col)\n",
        "            ans = deta.T @ deta\n",
        "            self.sigma.append(ans)\n",
        "\n",
        "        self.sigma = np.array(self.sigma)\n",
        "        #print(np.shape(self.sigma))\n",
        "        self.sigma = np.sum(self.sigma,0)\n",
        "        self.sigma = self.sigma/row\n",
        "        print(\"\\nFull cov matrix:\")\n",
        "        print(self.sigma)\n",
        "        self.sigma_diag = np.diag(np.diag(self.sigma))\n",
        "        print(\"\\nDiagonal cov matrix:\")\n",
        "        print(self.sigma_diag)\n",
        "        self.mu_positive = self.mu_positive.reshape(1,col)\n",
        "        self.mu_negetive = self.mu_negetive.reshape(1,col)\n",
        "\n",
        "    def Gaussian(self, x, mean, cov):\n",
        "        dim = np.shape(cov)[0]\n",
        "        # Cov measures of the determinant is zero\n",
        "        covdet = np.linalg.det(cov + np.eye(dim) * 0.001)\n",
        "        covinv = np.linalg.inv(cov + np.eye(dim) * 0.001)\n",
        "        xdiff = (x - mean).reshape((1, dim))\n",
        "        # Probability Density\n",
        "        prob = 1.0 / (np.power(np.power(2 * np.pi, dim) * np.abs(covdet), 0.5)) * \\\n",
        "               np.exp(-0.5 * xdiff @ covinv @ xdiff.T)[0][0]\n",
        "        return prob\n",
        "\n",
        "    def predict(self,test_data, co_var_mat):\n",
        "        predict_label = []\n",
        "        for data in test_data:\n",
        "          # predict based on full co-variance matrix.\n",
        "            if co_var_mat == 0:\n",
        "              class_B_prob = self.Gaussian(data,self.mu_positive,self.sigma)\n",
        "              class_A_prob = self.Gaussian(data,self.mu_negetive,self.sigma)\n",
        "            else:\n",
        "              class_B_prob = self.Gaussian(data,self.mu_positive,self.sigma_diag)\n",
        "              class_A_prob = self.Gaussian(data,self.mu_negetive,self.sigma_diag)\n",
        "              \n",
        "            if class_B_prob >= class_A_prob:\n",
        "                predict_label.append(1)\n",
        "            else:\n",
        "                predict_label.append(0)\n",
        "        return predict_label\n",
        "\n",
        "def run_main():\n",
        "\n",
        "    # GDA results\n",
        "    gda = GDA(Xtrain,ytrain)\n",
        "\n",
        "    #Predict for full co-variance matrix\n",
        "    y_full_pred = gda.predict(Xtest,0)\n",
        "    print(\"\\nGDA accuracy with full cov matrix:\",accuracy_score(ytest,y_full_pred))\n",
        "\n",
        "    #Predict for diagonal co-variance matrix\n",
        "    y_diag_pred = gda.predict(Xtest,1)\n",
        "    print(\"\\nGDA accuracy with diagonal cov matrix:\",accuracy_score(ytest,y_diag_pred))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class A mean:  [0.64449763 0.56028706 0.5430622 ]\n",
            "Class B mean:  [0.52718675 0.5505319  0.6524823 ]\n",
            "\n",
            "Full cov matrix:\n",
            "[[ 0.02126539 -0.00850425 -0.00650579]\n",
            " [-0.00850425  0.03082928  0.01286693]\n",
            " [-0.00650579  0.01286693  0.03073367]]\n",
            "\n",
            "Diagonal cov matrix:\n",
            "[[0.02126539 0.         0.        ]\n",
            " [0.         0.03082928 0.        ]\n",
            " [0.         0.         0.03073367]]\n",
            "\n",
            "GDA accuracy with full cov matrix: 0.6566265060240963\n",
            "\n",
            "GDA accuracy with diagonal cov matrix: 0.6602409638554216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v62JWxiweOJT"
      },
      "source": [
        "From the results, we observe that the multivariate Gaussian model does not fit the training data very well due to its high dimension and thus requires a more complex model. This can be verified with the accuracies obtained on the test data where the model performs slightly better with the diagonal covariance matrix.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkecIUfIS228"
      },
      "source": [
        "## Gaussian Mixture Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_xP4EHZgqmk"
      },
      "source": [
        "Description: <br> \n",
        "\n",
        "To improve the Gaussian classifier from the multivariate Gaussian model by using a Gaussian Mixture Model to\n",
        "model each class. <br>\n",
        "\n",
        "Investigate the Gaussian Mixture Models that have 2, 4, 8, or 16 Gaussian components and determine the best\n",
        "model configuration in terms of the number of Gaussian components and the covariance\n",
        "matrix structure (diagonal vs. full) for the given data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUKd8JswgB4V"
      },
      "source": [
        "Model Explanation: <br> \n",
        "\n",
        "Here, we implement Gaussian\n",
        "Mixture Models (GMMs) for the two classes in the dataset. We build a separate GMM for\n",
        "each class, and cluster each GMM into a given number of clusters. Next, we predict the class\n",
        "of each data in a given GMM using the MAP rule and evaluate the predictions against their\n",
        "true labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhWNRjbtgE8g"
      },
      "source": [
        "We initialize the model using the number of clusters and the number of training epochs. For\n",
        "faster training, the number of epochs is 1 here for which the model gives sufficiently good\n",
        "results. Next, we separate the training data according to their classes and initiate the model\n",
        "training for each class. The training process for each class includes initializing the mean\n",
        "vector, the covariance vector, and the weights vector using k-means clustering which will be\n",
        "used as starting points for the GMM. Next, the ’E’ and the ’M’ steps are performed for each\n",
        "class which update the phi and the weights variables in the ’E’ step, and the mean and the\n",
        "covariance variables in the ’M’ step. This process continues for a given number of epochs and\n",
        "return the mean vector, covariance matrix, and the diagonal covariance matrix upon\n",
        "completion. Thus, we obtain all three variables (mean, covariance, diagonal covariance) for\n",
        "each cluster in a given class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA2eFEAZgRJm"
      },
      "source": [
        "Now, to predict the class of each data point in the test data, we first predict the cluster that\n",
        "each data point belongs to. Once the class labels for each data point are obtained, we predict\n",
        "the probability of both classes on the given data point using the mean and variance of the\n",
        "given cluster. This mean and variance for a given cluster is more accurate since it contains more data points concentrated in a given region and hence the values are more likely to be\n",
        "closer to the true value. We thus predict the class label for each data point in the test set and\n",
        "evaluate against the true labels. We obtain the following results when we compare with\n",
        "various combinations of the model including 2, 4, 8, 16 Gaussian components, both with the\n",
        "full covariance matrix and the diagonal covariance matrix. A best of 5 runs was taken as the\n",
        "result for the experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-AA9yJKE15C",
        "outputId": "599dfac3-c85b-4464-9337-a1167feda352"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score    \n",
        "\n",
        "class GMM:\n",
        "    def __init__(self, train_data,train_label, k, max_iter=1):\n",
        "        self.k = k\n",
        "        self.max_iter = int(max_iter)\n",
        "\n",
        "        self.Train_Data = train_data\n",
        "        self.Train_Label = train_label\n",
        "        self.postive_num = 0                                                    \n",
        "        self.negetive_num = 0                                                   \n",
        "        postive_data = []                                                       \n",
        "        negetive_data = [] \n",
        "                                                            \n",
        "        for (data,label) in zip(self.Train_Data,self.Train_Label):\n",
        "            if label == 1:          \n",
        "                self.postive_num += 1\n",
        "                postive_data.append(list(data))\n",
        "            else:                   \n",
        "                self.negetive_num += 1\n",
        "                negetive_data.append(list(data))\n",
        "\n",
        "        self.pos_means, self.pos_sigma, self.pos_diag_sigma = self.fit(np.asarray(postive_data))\n",
        "        self.neg_means, self.neg_sigma, self.neg_diag_sigma = self.fit(np.asarray(negetive_data))\n",
        "\n",
        "    def cluster_assign(self, X):        \n",
        "        cluster_assignment = {i:[] for i in range(self.k)}\n",
        "        for point in X:\n",
        "            cluster_dist =[]\n",
        "            for cluster in self.mu:\n",
        "                cluster_dist.append(np.sqrt(np.sum((point-cluster)**2)))\n",
        "            located_cluster = cluster_dist.index(min(cluster_dist))\n",
        "            cluster_assignment[located_cluster].append(point)\n",
        "\n",
        "        groupby_points_previous = {cluster:len(points) for cluster,points in cluster_assignment.items()}        \n",
        "        return groupby_points_previous,cluster_assignment\n",
        "\n",
        "    def fit_kmeans(self, X):\n",
        "\n",
        "        self.sigma = []\n",
        "        self.diag_sigma = []\n",
        "        self.weights = []\n",
        "        self.mu = []\n",
        "    \n",
        "        np.random.shuffle(X)        \n",
        "        random_row = np.random.randint(low=0, high=self.n, size=self.k)\n",
        "        self.mu = np.asarray([ X[row_index,:] for row_index in random_row ])\n",
        "        grouped_cluster_prev, assigned_clusters_prev = self.cluster_assign(X)\n",
        "        \n",
        "        while True:      \n",
        "            groupby_points_next, assigned_clusters_new = self.cluster_assign(X)            \n",
        "            if groupby_points_next == grouped_cluster_prev:\n",
        "                break                \n",
        "            else:\n",
        "                 grouped_cluster_prev, assigned_clusters_prev = groupby_points_next, assigned_clusters_new\n",
        "\n",
        "        self.weights = np.full( shape=self.shape, fill_value=1/self.k)\n",
        "        for cluster, points in assigned_clusters_new.items():\n",
        "            temp = np.cov(np.asarray(points).T)\n",
        "            self.sigma.append(temp)\n",
        "\n",
        "        self.mu = np.asarray([ X[row_index,:] for row_index in random_row ])\n",
        "        self.sigma = np.asarray([ np.cov(X.T) for _ in range(self.k) ])\n",
        "        self.diag_sigma = np.asarray([ np.diag(np.diag(np.cov(X.T))) for _ in range(self.k) ])\n",
        "\n",
        "    def initialize(self, X):\n",
        "        self.shape = X.shape\n",
        "        self.n, self.m = self.shape\n",
        "        self.phi = np.full(shape=self.k, fill_value=1/self.k)\n",
        "        self.fit_kmeans(X) \n",
        "\n",
        "    def e_step(self, X):\n",
        "        self.weights = self.predict_proba(X)\n",
        "        self.phi = self.weights.mean(axis=0)\n",
        "    \n",
        "    def m_step(self, X):\n",
        "        for i in range(self.k):\n",
        "            weight = self.weights[:, [i]]\n",
        "            total_weight = weight.sum()\n",
        "            self.mu[i] = (X * weight).sum(axis=0) / total_weight\n",
        "            self.sigma[i] = np.cov(X.T, aweights=(weight/total_weight).flatten(), bias=True)\n",
        "            self.diag_sigma[i] = np.diag(np.diag(self.sigma[i]))\n",
        "\n",
        "    def fit(self, X):\n",
        "        self.initialize(X)\n",
        "        \n",
        "        for iteration in range(self.max_iter):\n",
        "            self.e_step(X)\n",
        "            self.m_step(X)\n",
        "        \n",
        "        return self.mu, self.sigma, self.diag_sigma\n",
        "            \n",
        "    def predict_proba(self, X):\n",
        "        likelihood = np.zeros( (X.shape[0], self.k) )\n",
        "\n",
        "        for i in range(self.k):\n",
        "            probab = []\n",
        "            for x in X:\n",
        "              probab.append(self.Gaussian(x, self.mu[i], self.sigma[i]))\n",
        "            probab_np = np.array(probab)\n",
        "            likelihood[:,i] = probab_np\n",
        "        \n",
        "        numerator = likelihood * self.phi\n",
        "        denominator = numerator.sum(axis=1)[:, np.newaxis]\n",
        "        weights = numerator / denominator\n",
        "        return weights\n",
        "    \n",
        "    def predict_class(self, X):\n",
        "        weights = self.predict_proba(X)\n",
        "        return np.argmax(weights, axis=1)\n",
        "\n",
        "    def Gaussian(self, x, mean, cov):\n",
        "        dim = np.shape(cov)[0]\n",
        "        # Cov measures of the determinant is zero\n",
        "        covdet = np.linalg.det(cov + np.eye(dim) * 0.001)\n",
        "        covinv = np.linalg.inv(cov + np.eye(dim) * 0.001)\n",
        "        xdiff = (x - mean).reshape((1, dim))\n",
        "        # Probability Density\n",
        "        prob = 1.0 / (np.power(np.power(2 * np.pi, dim) * np.abs(covdet), 0.5)) * np.exp(-0.5 * xdiff @ covinv @ xdiff.T)[0][0]\n",
        "        return prob\n",
        "\n",
        "    def predict(self,test_data, co_var_mat):\n",
        "        predict_label = []\n",
        "\n",
        "        pred_class = self.predict_class(test_data)\n",
        "\n",
        "        for data, data_class in zip(test_data, pred_class):\n",
        "\n",
        "            # co_var_mat = 0 -> Full covariance matrix\n",
        "            if co_var_mat == 0:\n",
        "                class_B_prob = self.Gaussian(data,self.pos_means[data_class],self.pos_sigma[data_class])\n",
        "                class_A_prob = self.Gaussian(data,self.neg_means[data_class],self.neg_sigma[data_class])\n",
        "\n",
        "            # co_var_mat != 0 -> Diagonal covariance matrix\n",
        "            else:\n",
        "                class_B_prob = self.Gaussian(data,self.pos_means[data_class],self.pos_diag_sigma[data_class])\n",
        "                class_A_prob = self.Gaussian(data,self.neg_means[data_class],self.neg_diag_sigma[data_class])\n",
        "              \n",
        "            if class_B_prob >= class_A_prob:\n",
        "                predict_label.append(1)\n",
        "            else:\n",
        "                predict_label.append(0)\n",
        "        return predict_label\n",
        "\n",
        "def run_main():\n",
        "\n",
        "    for i in range(4):\n",
        "\n",
        "        gmm = GMM(Xtrain,ytrain, 2**(i+1), 1)\n",
        "\n",
        "        # TRAIN accuracies.\n",
        "        # #Predict for full co-variance matrix\n",
        "        # ytrain_full_pred = gmm.predict(Xtrain, 0)\n",
        "        # print(\"\\nGMM train accuracy with\",2**(i+1),\" Gaussian components and full covariance matrix:\",accuracy_score(ytrain, ytrain_full_pred))\n",
        "\n",
        "        # #Predict for diagonal co-variance matrix\n",
        "        # ytrain_diag_pred = gmm.predict(Xtrain, 1)\n",
        "        # print(\"\\nGMM train accuracy with\",2**(i+1),\" Gaussian components and diagonal covariance matrix:\",accuracy_score(ytrain,ytrain_diag_pred))\n",
        "\n",
        "        # TEST accuracies.\n",
        "        #Predict for full co-variance matrix\n",
        "        ytest_full_pred = gmm.predict(Xtest, 0)\n",
        "        print(\"\\nGMM test accuracy with\",2**(i+1),\" Gaussian components and full covariance matrix:\",accuracy_score(ytest, ytest_full_pred))\n",
        "\n",
        "        #Predict for diagonal co-variance matrix\n",
        "        ytest_diag_pred = gmm.predict(Xtest, 1)\n",
        "        print(\"\\nGMM test accuracy with\",2**(i+1),\" Gaussian components and diagonal covariance matrix:\",accuracy_score(ytest,ytest_diag_pred))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "GMM test accuracy with 2  Gaussian components and full covariance matrix: 0.7542168674698795\n",
            "\n",
            "GMM test accuracy with 2  Gaussian components and diagonal covariance matrix: 0.7409638554216867\n",
            "\n",
            "GMM test accuracy with 4  Gaussian components and full covariance matrix: 0.7674698795180723\n",
            "\n",
            "GMM test accuracy with 4  Gaussian components and diagonal covariance matrix: 0.6120481927710844\n",
            "\n",
            "GMM test accuracy with 8  Gaussian components and full covariance matrix: 0.7373493975903614\n",
            "\n",
            "GMM test accuracy with 8  Gaussian components and diagonal covariance matrix: 0.7168674698795181\n",
            "\n",
            "GMM test accuracy with 16  Gaussian components and full covariance matrix: 0.8\n",
            "\n",
            "GMM test accuracy with 16  Gaussian components and diagonal covariance matrix: 0.7843373493975904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-AFeeG_gTNM"
      },
      "source": [
        "As we see in the results, the GMM model performed much better than the multivariate\n",
        "Gaussian models since they were able to model the data in higher dimensions and thus\n",
        "compute accurate classifications. The model was also trained for only one epoch for this\n",
        "experiment for which the GMMs are able to provide sufficiently good results. We also notice\n",
        "that among all the different number of clusters and thus their Gaussian components, we find\n",
        "that GMMs performed best with 4 Gaussian components followed by 8, 2, and 16 for this\n",
        "dataset. Thus, we can conclude that the mixture models are better able to capture the features\n",
        "of the training data as compared to multivariate Gaussian models and thus give a better\n",
        "accuracy/ result on the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUjGrZLaO6rM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}